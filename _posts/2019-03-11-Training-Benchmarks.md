---
layout: post
title: Training Benchmark
date: 2019-03-11 4:30:00
---

##Introduction
	I learned Convolutional Neural Networks from the point of view of a
technician. As an undergrad, I joined a research lab that had begun
using CNN's for their work. I was tasked with converting a
classification task into a regression. Most of the model architecture
was fixed and already built in Caffe. I was to play with the output
layer and the data pipeline to get continuous values as labels. The
results of that are iinteresting, but left for another post.

	As a result of this, I learned to use CNN's through a series of
rules-of-thumb. As I play more with architecture and hyperparameters, I
come into conflict with these rules and, in a series of posts, I'd like
to see where each one comes from.

##The Rule
The first rule-of-thumb I questioned was *It's better if your batch size
is always a power of two*.

###The Intuition



[this StackExchange Post](https://datascience.stackexchange.com/questions/20179/what-is-the-advantage-of-keeping-batch-size-a-power-of-2)

##Is it worth it?

Butts and stuff like that
